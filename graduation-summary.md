# 大模型微调训练营0期毕业总结
在《大模型微调训练营0期》（以下简称“微调训练营”）时长8周的学习进入尾声时，对这段学习做简单的总结。

### 参加微调训练营的原因
在参加微调训练营之前，我曾经在2023年7月参加《大模型应用训练营0期》（以下简称“应用训练营”）的学习并顺利毕业，在应用训练营中我了解了OpenAI API、Langchain等应用开发知识，在学习过程中对大语言模型的微调萌生了浓厚的兴趣，期待使用微调的方式对原有大模型的能力进行增强，在得到微调训练营即将开始的消息时选择加入。

### 课程感受
训练营中的课程安排依然保持了循序渐进的风格，从Transformers库、全量微调、模型量化、高效微调逐渐展开，彭老师在充分讲解理论的前提下辅以有效的实践，并在课程进行中加入一些新技术的观点、见解。在完成各个章节的作业之后能对微调有不错的理解，与应用训练营中的知识相辅相成。

我的情况比较另类，之前曾在“得到app”中万维钢老师专栏中了解过GPT-3，2022年之前一直从事软件开发工作主要负责Java后端，从2022年初开始没有工作，并于2022年末开始使用AI绘画模型，进而开始关注ChatGPT这类基于大语言模型构建的应用。在参加了应用训练营之后我对智能体应用有了深入的理解具备了开发智能体的能力，本次的微调训练营将我对基于大模型应用的理解加深了，掌握了更多的技术手段来构建符合需求的大模型与应用。期待在2023年开启的“AI元年”之后进入大模型领域开始新的职业生涯。

### 建议
我的水平有限，建议之后微调训练营的同学要有耐心并及时开始实践作业，微调的时间成本较高，不要累积问题到最后。
另外，由于大模型技术日新月异的发展速度，我很喜欢彭老师在课程中逐渐加入新内容，期待彭老师继续“挖坑必填”的风格。
